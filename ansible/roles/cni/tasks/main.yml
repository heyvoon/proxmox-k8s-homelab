---
# 0) Pull kubeconfig from the first control-plane (remote)
- name: Slurp /etc/kubernetes/admin.conf from control-plane
  slurp:
    src: /etc/kubernetes/admin.conf
  register: adminconf
  delegate_to: "{{ groups['masters'][0] }}"
  become: true
  run_once: true

# 1) Prepare local kubeconfig on the controller
- name: Ensure local .kube exists beside the playbook
  ansible.builtin.file:
    path: "{{ playbook_dir }}/.kube"
    state: directory
    mode: "0700"
  delegate_to: localhost
  become: false
  run_once: true

- name: Write kubeconfig locally for modules
  ansible.builtin.copy:
    dest: "{{ playbook_dir }}/.kube/admin.conf"
    content: "{{ adminconf.content | b64decode }}"
    mode: "0600"
  delegate_to: localhost
  become: false
  run_once: true

- name: Set kubeconfig path fact
  set_fact:
    kubeconfig_local: "{{ playbook_dir }}/.kube/admin.conf"
  delegate_to: localhost
  run_once: true

# 2) Everything below runs on the controller (localhost) without sudo
- name: Controller-local Helm setup and deployments
  block:

    # 2.1) Install Helm locally (no sudo)
    - name: Ensure local bin dir for helm
      ansible.builtin.file:
        path: "{{ playbook_dir }}/.bin"
        state: directory
        mode: "0755"

    - name: Download Helm v{{ helm_version }}
      ansible.builtin.get_url:
        url: "https://get.helm.sh/helm-v{{ helm_version }}-linux-amd64.tar.gz"
        dest: /tmp/helm.tar.gz
        mode: "0644"

    - name: Extract Helm
      ansible.builtin.unarchive:
        src: /tmp/helm.tar.gz
        dest: /tmp
        remote_src: true

    - name: Install Helm binary into project bin (no sudo)
      ansible.builtin.copy:
        src: /tmp/linux-amd64/helm
        dest: "{{ playbook_dir }}/.bin/helm"
        mode: "0755"
        remote_src: true

    - name: Set helm binary path fact
      set_fact:
        helm_bin: "{{ playbook_dir }}/.bin/helm"

    # 2.2) Wait for API readiness
    # Ensure the Python Kubernetes client is present on the controller
    - name: Install Python 'kubernetes' client on controller (using the same interpreter as ansible-playbook)
      command: >
        {{ ansible_playbook_python }} -m pip install --user --upgrade
        "kubernetes>=27.0.0" "pyyaml>=6.0.1" "requests>=2.31.0"
      delegate_to: localhost
      become: false
      run_once: true
      environment:
        PIP_DISABLE_PIP_VERSION_CHECK: "1"
        PIP_ROOT_USER_ACTION: "ignore"

    # Make delegated localhost tasks use the same python
    - name: Pin localhost interpreter to ansible_playbook_python
      set_fact:
        _noop: true
      delegate_to: localhost
      run_once: true
      vars:
        ansible_python_interpreter: "{{ ansible_playbook_python }}"

    - name: Wait for Kubernetes API
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Namespace
        name: kube-system
        kubeconfig: "{{ kubeconfig_local }}"
      register: ks
      retries: 30
      delay: 5
      until: ks.resources is not none

    # 2.3) Add Helm repositories (force update = refresh indices)
    - name: Add Helm repos
      kubernetes.core.helm_repository:
        name: "{{ item.name }}"
        repo_url: "{{ item.url }}"
        kubeconfig: "{{ kubeconfig_local }}"
        force_update: true
        binary_path: "{{ helm_bin }}"
      loop:
        - { name: ingress-nginx,         url: https://kubernetes.github.io/ingress-nginx }
        - { name: argo,                  url: https://argoproj.github.io/argo-helm }
        - { name: portainer,             url: https://portainer.github.io/k8s/ }
        - { name: prometheus-community,  url: https://prometheus-community.github.io/helm-charts }
        - { name: grafana,               url: https://grafana.github.io/helm-charts }
        - { name: metallb,               url: https://metallb.github.io/metallb }
        - { name: cilium,                url: https://helm.cilium.io/ }
      delegate_to: localhost
      run_once: true

    # 2.4) MetalLB (install chart, then CRs)
# Ensure MetalLB is installed (helm or manifests) BEFORE these tasks

    - name: Add Cilium repo
      kubernetes.core.helm_repository:
        name: cilium
        repo_url: https://helm.cilium.io

    - name: Install/upgrade Cilium
      kubernetes.core.helm:
        name: cilium
        chart_ref: cilium/cilium
        release_namespace: kube-system
        create_namespace: false
        update_repo_cache: true
        wait: true
        timeout: 600
        values: "{{ cilium_values }}"
      delegate_to: localhost

    - name: Resolve MetalLB address list (accept list, cidr, or range)
      set_fact:
        _metallb_addresses: >-
          {{
            (metallb_addresses if (metallb_addresses | default([])) | length > 0 else [])
            + ([metallb_pool_cidr] if (metallb_pool_cidr | default('')) | length > 0 else [])
            + ([metallb_ip_range] if (metallb_ip_range is defined and metallb_ip_range | length > 0) else [])
          }}
      delegate_to: localhost
      run_once: true

    - name: Validate MetalLB address list
      assert:
        that: _metallb_addresses | length > 0
        fail_msg: "Provide metallb_addresses (list) or metallb_pool_cidr/metallb_ip_range (string)."
      delegate_to: localhost
      run_once: true

    - name: Configure MetalLB IPAddressPool
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: metallb.io/v1beta1
          kind: IPAddressPool
          metadata:
            name: "{{ metallb_pool_name }}"
            namespace: "{{ metallb_namespace }}"
          spec:
            addresses: "{{ _metallb_addresses }}"
      delegate_to: localhost
      run_once: true

    - name: Configure MetalLB L2Advertisement
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: metallb.io/v1beta1
          kind: L2Advertisement
          metadata:
            name: "{{ metallb_pool_name }}"
            namespace: "{{ metallb_namespace }}"
          spec:
            ipAddressPools: ["{{ metallb_pool_name }}"]
      delegate_to: localhost
      run_once: true

    # Explicitly wait for controller & speaker to be Ready
    - name: Wait for MetalLB controller and speaker pods to be Ready
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: metallb-system
        label_selectors:
          - "app.kubernetes.io/name in (metallb,controller,speaker)"
      register: mlb_pods
      retries: 30
      delay: 10
      until: >
        (mlb_pods.resources | selectattr('status.phase','equalto','Running') | list | length) >= 2 and
        (mlb_pods.resources | map(attribute='status.containerStatuses') | list
          | map('selectattr','ready') | list) is not none
      delegate_to: localhost
      run_once: true

    # 2.5) Ingress-NGINX
    - name: Install ingress-nginx
      kubernetes.core.helm:
        name: ingress-nginx
        chart_ref: ingress-nginx/ingress-nginx
        release_namespace: ingress-nginx
        create_namespace: true
        wait: true
        kubeconfig: "{{ kubeconfig_local }}"
        binary_path: "{{ helm_bin }}"
        values:
          controller:
            service: { type: LoadBalancer }
            metrics: { enabled: true }
      delegate_to: localhost
      run_once: true


    # 2.6) Argo CD
    - name: Install Argo CD
      kubernetes.core.helm:
        name: argocd
        chart_ref: argo/argo-cd
        release_namespace: argocd
        create_namespace: true
        wait: true
        kubeconfig: "{{ kubeconfig_local }}"
        binary_path: "{{ helm_bin }}"
        values:
          server:
            service: { type: ClusterIP }
            ingress:
              enabled: true
              ingressClassName: nginx
              hosts: [ "argocd.{{ hostvars[groups['masters'][0]].ansible_default_ipv4.address }}.nip.io" ]
              tls:
                - secretName: argocd-server-tls
                  hosts: [ "argocd.{{ hostvars[groups['masters'][0]].ansible_default_ipv4.address }}.nip.io" ]
          configs:
            params: { server.insecure: true }

    - name: Show Argo CD admin password
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Secret
        name: argocd-initial-admin-secret
        namespace: argocd
        kubeconfig: "{{ kubeconfig_local }}"
      register: argocd_secret

    - name: Argo CD admin password (decoded)
      debug:
        msg: "{{ (argocd_secret.resources[0].data.password | b64decode) | default('not found') }}"

    # 2.7) Portainer
    - name: Install Portainer
      kubernetes.core.helm:
        name: portainer
        chart_ref: portainer/portainer
        release_namespace: portainer
        create_namespace: true
        wait: true
        kubeconfig: "{{ kubeconfig_local }}"
        binary_path: "{{ helm_bin }}"
        values:
          service: { type: ClusterIP }
          ingress:
            enabled: true
            ingressClassName: nginx
            hosts:
              - host: "portainer.{{ hostvars[groups['masters'][0]].ansible_default_ipv4.address }}.nip.io"
                paths: [ { path: "/", pathType: Prefix } ]
            tls:
              - secretName: portainer-tls
                hosts: [ "portainer.{{ hostvars[groups['masters'][0]].ansible_default_ipv4.address }}.nip.io" ]

    # 2.8) kube-prometheus-stack
    - name: Install kube-prometheus-stack
      kubernetes.core.helm:
        name: kube-prometheus-stack
        chart_ref: prometheus-community/kube-prometheus-stack
        release_namespace: monitoring
        create_namespace: true
        wait: true
        kubeconfig: "{{ kubeconfig_local }}"
        binary_path: "{{ helm_bin }}"
        values:
          grafana:
            adminPassword: "{{ grafana_admin_password }}"
            service: { type: ClusterIP }
            ingress:
              enabled: true
              ingressClassName: nginx
              hosts: [ "grafana.{{ hostvars[groups['masters'][0]].ansible_default_ipv4.address }}.nip.io" ]
              tls:
                - secretName: grafana-tls
                  hosts: [ "grafana.{{ hostvars[groups['masters'][0]].ansible_default_ipv4.address }}.nip.io" ]
          prometheus:
            service: { type: ClusterIP }
            ingress:
              enabled: true
              ingressClassName: nginx
              hosts: [ "prometheus.{{ hostvars[groups['masters'][0]].ansible_default_ipv4.address }}.nip.io" ]

    # 2.9) Loki stack
    - name: Install Loki stack
      kubernetes.core.helm:
        name: loki-stack
        chart_ref: grafana/loki-stack
        release_namespace: logging
        create_namespace: true
        wait: true
        kubeconfig: "{{ kubeconfig_local }}"
        binary_path: "{{ helm_bin }}"
        values:
          loki:
            persistence: { enabled: true, size: 10Gi }
          promtail: { enabled: true }
          grafana: { enabled: false }

  delegate_to: localhost
  become: false
  run_once: true
